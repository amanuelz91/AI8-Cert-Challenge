Task #1
1. Write a succinct 1-sentence description of the problem
A: It‚Äôs difficult to get clear, accurate answers to questions about student loans and repayment‚Äîwhether you‚Äôre a current or prospective borrower, or even a customer service agent working at a federal student loan organization like MOHELA or Federal Student Aid.
2. Write 1-2 paragraphs on why this is a problem for your specific user
A: The lack of clear, consistent, and accessible information about student loans is a major problem for millions of Americans navigating repayment. As payments resume and policy changes continue to shift, borrowers are turning to servicers like MOHELA and Federal Student Aid for guidance‚Äîyet even customer service agents within these organizations struggle to provide accurate answers due to outdated systems, fragmented data, and constantly evolving regulations. This confusion not only leads to long call wait times and miscommunication but also deepens financial anxiety and mistrust among borrowers. With $1.777 trillion in total student loan debt as of Q1 2025‚Äî$1.693 trillion of which is federal‚Äîthis issue affects roughly 42.7 million people in the U.S., and call volume is expected to surge throughout 2025 as more borrowers seek help understanding repayment options, forgiveness programs, and interest accrual.
3. Success: 
A: üß™ Hypothesis: Current tools for searching, retrieving, and generating helpful responses do not meaningfully increase an agent‚Äôs efficiency or capacity to handle high volumes of customer complaints each day.
4. Audience:
A: We are building this application for customer service agents who work within federal student loan organizations, such as MOHELA or Federal Student Aid. These agents handle thousands of inquiries daily from borrowers seeking clarity on repayment options, forgiveness programs, and policy changes. Our goal is to empower them with faster, more accurate tools for retrieving and generating information‚Äîreducing time spent searching through fragmented databases and enabling them to provide clear, confident answers to borrowers in need.

Task 2: Propose a Solution

1. Write 1-2 paragraphs on your proposed solution.  How will it look and feel to the user?

A: Our solution is a production-grade RAG (Retrieval-Augmented Generation) system that provides customer service agents with an intuitive, fast, and reliable interface for answering student loan questions. The system feels like having an expert assistant by their side‚Äîagents simply type their question in a clean interface, and within seconds they receive a comprehensive answer backed by official policy documents, complete with source citations. The API supports both synchronous query responses and real-time streaming, so agents can see answers generate progressively for longer queries. Beyond simple Q&A, the system intelligently selects from multiple retrieval strategies‚Äîfrom fast semantic search over knowledge base documents to real-time web search via Tavily for questions not in the knowledge base. The interface provides confidence scores, source documents, and detailed metadata about which retrieval method was used, empowering agents to verify information and respond with full transparency to borrowers.

2. Describe the tools you plan to use in each part of your stack.  Write one sentence on why you made each tooling choice.
    1. LLM
    A: We use **GPT-4o-mini** as our primary LLM because it offers the best balance of cost, speed, and reasoning quality for customer support use cases, enabling us to handle high query volumes without compromising response accuracy.
    
    2. Embedding Model
    A: We use **OpenAI's text-embedding-3-small** because it provides high-quality semantic representations with 1536 dimensions at a fraction of the cost of larger models, perfectly matching our vector database configuration and throughput requirements.
    
    3. Orchestration
    A: We use **LangChain LCEL** for declarative chain composition and **LangGraph** for complex multi-step workflows because they provide clean abstractions for combining retrieval, generation, and tool calls while maintaining production-grade error handling and observability.
    
    4. Vector Database
    A: We use **Qdrant** as our vector database because it offers production-ready performance, efficient similarity search at scale, and cloud deployment options that match enterprise requirements for reliability and compliance.
    
    5. Monitoring
    A: We use **LangSmith** for observability because it provides comprehensive tracing of the entire RAG pipeline, from retrieval through generation, enabling us to debug issues, optimize performance, and ensure response quality in production.
    
    6. Evaluation
    A: We use **RAGAS** (Retrieval-Augmented Generation Assessment) for evaluation because it provides automated, metrics-driven assessment of context recall, faithfulness, and answer relevancy without requiring manual labeling for every test case.
    
    7. User Interface
    A: We use **FastAPI** for our REST API because it provides automatic OpenAPI documentation, native async/await support for concurrent queries, built-in validation with Pydantic, and easy streaming support for real-time responses.
    
    8. (Optional) Serving & Inference
    A: We use **Uvicorn** as our ASGI server because it provides excellent performance for async workloads, native HTTP/2 support, and easy production deployment with process management and reload capabilities.

3. Where will you use an agent or agents?  What will you use "agentic reasoning" for in your app?

A: We do not use explicit agentic frameworks or autonomous agents in our application. Instead, our system uses **tool-based retrieval** where the system intelligently selects and invokes external tools (like Tavily web search) when questions fall outside our knowledge base. The "reasoning" happens through our hybrid retrieval strategy‚Äîwhen a query comes in, the system retrieves from both the vector database and optional web search, then synthesizes these different information sources into a coherent answer. If we were to add agentic reasoning in the future, we would use it to dynamically route between retrieval methods based on question complexity (e.g., using a router agent to decide whether a query needs web search vs. knowledge base lookup), and to handle multi-step follow-up questions where the agent maintains conversation context and can ask clarifying questions before providing final answers.

We want to build a system that can intelligently answer real-world questions like these:

| **User Type**                        | **Scenario / Example Question**                                                                                                                 |
| ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| **Current borrowers (in repayment)** | A borrower says they‚Äôve fallen three months behind on payments and can‚Äôt afford to catch up. What‚Äôs the most effective course of action?        |
| **Current borrowers (in repayment)** | A borrower insists they already submitted their Income-Driven Repayment (IDR) renewal form, but no record exists. How should the agent proceed? |
| **Current borrowers (in repayment)** | A borrower argues, ‚ÄúI shouldn‚Äôt have to repay these loans ‚Äî this is unconstitutional.‚Äù How can the agent respond professionally and accurately? |
| **Prospective borrowers**            | Is taking out a federal student loan in 2025 a financially smart decision?                                                                      |
| **Prospective borrowers**            | How much can a student currently borrow from the government to attend college? Is there a maximum limit?                                        |
| **Prospective borrowers**            | What grants and scholarships are available that don‚Äôt need to be repaid?                                                                        |



Task 3: Dealing with the Data

1. Describe all of your data sources and external APIs, and describe what you'll use them for.

**For our initial RAG prototype**, we'll use a curated selection of **official policy documents** from the [Federal Student Aid Handbook (2025‚Äì2026)](https://fsapartners.ed.gov/knowledge-center/fsa-handbook/pdf/2025-2026). These resources provide the foundational rules and procedures that govern federal student aid programs:

* [**Application and Verification Guide**](https://fsapartners.ed.gov/sites/default/files/2025-2026/2025-2026_Federal_Student_Aid_Handbook/_knowledge-center_fsa-handbook_2025-2026_application-and-verification-guide.pdf) ‚Äî covers application processes, data verification, and eligibility documentation.
* [**Volume 3: Academic Calendars, Cost of Attendance, and Packaging**](https://fsapartners.ed.gov/sites/default/files/2025-2026/2025-2026_Federal_Student_Aid_Handbook/_knowledge-center_fsa-handbook_2025-2026_vol3.pdf) ‚Äî defines institutional calendars, cost calculations, and aid packaging standards.
* [**Volume 7: The Federal Pell Grant Program**](https://fsapartners.ed.gov/sites/default/files/2025-2026/2025-2026_Federal_Student_Aid_Handbook/_knowledge-center_fsa-handbook_2025-2026_vol7.pdf) ‚Äî outlines rules, calculations, and disbursement procedures for Pell Grants.
* [**Volume 8: The Direct Loan Program**](https://fsapartners.ed.gov/sites/default/files/2025-2026/2025-2026_Federal_Student_Aid_Handbook/_knowledge-center_fsa-handbook_2025-2026_vol8.pdf) ‚Äî details policies and requirements for federal Direct Loans, including repayment and servicing.

Additionally, we integrate **Tavily Search API** as an external tool for real-time web search. This allows our system to retrieve up-to-date information that may not be in our knowledge base‚Äîparticularly useful for questions about recent policy changes, current loan limits, or emerging programs that may not yet be reflected in the static handbook documents.

2. Describe the default chunking strategy that you will use.  Why did you make this decision?

A: We use **RecursiveCharacterTextSplitter** with a chunk size of **750 characters** and a chunk overlap of **100 characters**. This strategy splits documents hierarchically across natural boundaries (paragraphs, sentences, words) to preserve semantic coherence. We chose 750 characters because it's large enough to capture meaningful context (typically 2-4 sentences) while remaining small enough to fit multiple chunks within LLM context windows for hybrid retrieval. The 100-character overlap ensures continuity between chunks, preventing important information from being split across chunk boundaries‚Äîcritical for policy documents where a single sentence might span a chunk edge. For more advanced use cases, we also support **SemanticChunker** which identifies semantic breakpoints rather than fixed character counts, but we default to RecursiveCharacterTextSplitter for its speed and reliability in production.

3. [Optional] Will you need specific data for any other part of your application?   If so, explain.

A: Beyond the initial policy documents, we plan to incorporate **student loan complaint data** from the Consumer Financial Protection Bureau (CFPB) as a validation and training corpus. This would help us understand common borrower pain points and ensure our system addresses real-world scenarios. We may also ingest **historical FSA policy updates** to enable temporal reasoning (e.g., "What were the rules last year vs. this year?"), though this requires more sophisticated metadata management. For evaluation purposes, we maintain a test suite of Q&A pairs derived from actual customer service transcripts to measure retrieval quality, response accuracy, and confidence calibration across different question types.
